# -*- coding: utf-8 -*-
"""ANN_diagnosis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wZa0qKCzSkp3CtPfk7WN8wMILnbBgoew
"""

#import libraries
import glob 
from keras.models import Sequential, load_model
import numpy as np
import pandas as pd
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import matplotlib.pyplot as plt
import keras as k

from google.colab import files
uploaded = files.upload()
df = pd.read_csv('dataset.csv',sep=';')

#print the first 5 rows

print(df)

df.head()



#create a list of column names to keep
columns_to_retain = ['Elementary lesion','Homogene', 'Reticulated', 'Age', 'Sex', 'Tobacco', 'Stupefies', 'Alcohol', 'Diagnosis', 'Dysplasia']

#drop the columns that are not in columns_to_retain
df = df.drop( [col for col in df.columns if not col in columns_to_retain], axis=1 )

#drop the rows with na or missing values
df = df.dropna(axis=0)
df.tail()

#transfrom the non-numeric data in the columns

for column in df.columns:
  if df[column].dtype == np.number:
    continue
  else:
    df[column] = LabelEncoder().fit_transform( df[column])

df.head()

#split the data into independent (x) dataset (the features) and dependent (y) dataset (the target)
X = df.drop(['Diagnosis'], axis=1)
y1 = df['Diagnosis']
y = pd.get_dummies(y1).values

#split the data into 80% training and 20% testing & shuffle
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=True)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#build the model
model = Sequential()
model.add( Dense(9, input_shape= (9,), activation='relu'))
model.add( Dense(39, activation='softmax') )

#compile the model
model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#train the model


history = model.fit(X_train, y_train, epochs = 2000, batch_size= X_train.shape[0])

model.save('diagnosis.model')

#Visualize the model loss and accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['loss'])
plt.title('model accuracy and loss')
plt.ylabel('accuracy and loss')
plt.xlabel('epoch')

#Get the shape of the training and testing data set
print('shape of training data:', X_train.shape)
print('shape of test data:', X_test.shape)

score = model.evaluate(X_test, y_test, batch_size=X_train.shape[0])