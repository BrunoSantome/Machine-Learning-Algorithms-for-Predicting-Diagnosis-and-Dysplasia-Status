# -*- coding: utf-8 -*-
"""KNN_diagnosis_dysplasia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r-keeOB2S98rwjjUXvmGAmiDGbGmHsgB

# Machine learning modules

## z

Manipulating `numpy` arrays is an important part of doing machine learning (or, really, any type of scientific computation) in python. Numpy arrays are similar to Python lists but have much more functionality and are used mathematically as matrices.
"""

import pandas as pd
import numpy as np

"""Importing and exporting is very easy with CSV or other formats"""

#load the data
from google.colab import files
uploaded = files.upload()

df = pd.read_csv('DATA.csv',sep=';')

#print the first 5 rows
df.head()
print(df.size)
print(df.isna().any())

#Replace NaN elements with 0
df=df.fillna(0)

df.columns

df.values

df['Elementary lesion'].unique()

#df['Diagnosis'].unique()

df2=df.copy()
for index in df.columns:
    keys_list=range(0, df[index].unique().size) 
    values_list=list(df[index].unique()) 
    zip_iterator = zip(values_list, keys_list)
    column_dict = dict(zip_iterator)
    df2[index] = [column_dict[index] for index in df2[index]]

df2.describe()

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline
from matplotlib import pyplot as plt
#Using Pearson Correlation
plt.figure(figsize=(12,10))
cor = df2.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

#del df2['Well_Delimited']
#Correlation with output variable
cor_target = abs(cor["Diagnosis"])#Selecting highly correlated features
relevant_features = cor_target[cor_target>0.1]
print (relevant_features)

X=df2[df2.columns[0:-2]] 
Y1=df2[df2.columns[-2]] 
Y2=df2[df2.columns[-1]]

X.columns

import statsmodels.api as sm
X_1 = sm.add_constant(X)#Fitting sm.OLS model
model = sm.OLS(Y1,X_1).fit()
print(model.pvalues)
zip_OLS = zip(X.columns, model.pvalues.values[1:])
dict_OLS = dict(zip_OLS)
print(dict_OLS)

weak_correlation = dict()
p_max=0.05
for (key, value) in dict_OLS.items(): 
    if value > p_max:
        weak_correlation[key] = value
print(weak_correlation)

for (key, value) in weak_correlation.items():
    del X[key]
print(X.columns)

from sklearn.decomposition import PCA
pca = PCA()

pca.fit(X)

X_pca = pca.transform(X)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=Y1, linewidths=0, s=30)
plt.xlabel("first principal component")
plt.ylabel("second principal component");

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

knn = KNeighborsClassifier(n_neighbors=1)

# Y1 as a target
X_train, X_test, y_train, y_test = train_test_split(X_pca, Y1,
                                                    test_size=0.25,
                                                    random_state=1234)
knn.fit(X_train, y_train);

knn.score(X_test, y_test)

# Y2 as a target
X_train, X_test, y_train, y_test = train_test_split(X_pca, Y2,
                                                    test_size=0.25,
                                                    random_state=1234)

knn.fit(X_train, y_train);

knn.score(X_test, y_test)